{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "#### Review  / Questions [30 minutes]\n",
    "1. See No08 TUT and Nov11 LEC\n",
    "2. Review linear regression at the same time introduce train test verification concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f6a540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.780986703298156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train_test verification separate \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(131)\n",
    "from sklearn import datasets\n",
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, columns = cancer_data.feature_names)\n",
    "training_indices = cancer_df.sample(frac=.80, replace=False).index.sort_values()\n",
    "testing_indices = cancer_df.index[~cancer_df.index.isin(training_indices)]\n",
    "# now we use part of the data train SLRM and verify with it with the other part of the data \n",
    "import statsmodels.formula.api as smf\n",
    "SLRM = smf.ols('Q(\"mean compactness\") ~ Q(\"mean radius\") + Q(\"mean concavity\")', \n",
    "                                         data=cancer_df.loc[training_indices,:])\n",
    "SLRM_fit = SLRM.fit() # creates line of best fit\n",
    "np.corrcoef(SLRM_fit.predict(cancer_df.loc[testing_indices,:]),\n",
    "            cancer_df.loc[testing_indices,\"mean compactness\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00b55b",
   "metadata": {},
   "source": [
    "#### Demo [45 minutes] \n",
    "> Concept only, we will reserve coding for lecture \n",
    "1. decision tree(Introduce basic concept of decision tree)\n",
    "2. Create confusion matrix, explain how to read it \n",
    "3. Introduce concept of Sensitivity, Specificity, Accuracy, Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8b51b",
   "metadata": {},
   "source": [
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230424141242/dcr.png\" alt=\"Optional Title\" style=\"width: 500px; height: auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24650092",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:667/1*3yGLac6F4mTENnj5dBNvNQ.jpeg\" alt=\"Optional Title\" style=\"width: 500px; height: auto;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73746029",
   "metadata": {},
   "source": [
    "##### Sensitivity (True Positive Rate)\n",
    "Sensitivity measures the proportion of actual positives that are correctly identified.\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "##### Specificity (True Negative Rate)\n",
    "Specificity measures the proportion of actual negatives that are correctly identified.\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "##### Accuracy\n",
    "Accuracy measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "##### Precision (Positive Predictive Value)\n",
    "Precision measures the proportion of positive identifications that were actually correct.\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce69a7",
   "metadata": {},
   "source": [
    "#### Communication \n",
    "\n",
    "\n",
    "1. **[15 minutes]** Break into 5 groups of 5 and prepare a speech describing what is the purpose of train test verification\n",
    "   \n",
    "2. **[20 minutes]** Which parameter is the most important?  Sensitivity, Specificity, Accuracy, or Precision. Use GPT to answer this question and then discuss with group member about your conclusion.\n",
    "\n",
    "\n",
    "#### Homework  -- To be completed before lecture--\n",
    "\n",
    "> Code and write all your answers in a python notebook (in code and markdown cells) and save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking.\n",
    "\n",
    "1. Understanding Decision Trees\n",
    "- \"Explain Decision Trees: Write a brief paragraph explaining what a decision tree is. Include in your response a description of how decision trees are used in data analysis, the type of problems they can solve, and how they make decisions at each node. Provide an example of a real-world application where decision trees might be particularly useful.\"\n",
    "\n",
    "2. Importance of Model Evaluation Metrics\n",
    "- \"Evaluate the Importance of Different Model Evaluation Metrics: For each of the following metrics - Sensitivity, Specificity, Accuracy, and Precision - provide a scenario where focusing on that particular metric is crucial. Explain why that metric is important in your given scenario and how it influences the decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4893ab",
   "metadata": {},
   "source": [
    "#### Homework  -- To be completed after lecture--\n",
    "\n",
    "> Code and write all your answers in a python notebook (in code and markdown cells) and save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bd64b",
   "metadata": {},
   "source": [
    "### Q0:  We begin by importing dataset and the libraries we will use. For each import write one comment explaining what it is. Then play aroud with this data to see what is it.\n",
    "#### Remember if you don't the answer, you can always use GPT or other resources. Just make sure you understand them in the end..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import graphviz as gv\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "ab = pd.read_csv(\"amazonbooks.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed4301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0444816",
   "metadata": {},
   "source": [
    "### Q1: Create a new dataframe `ab_noNaN` not containing the columns below and all rows with `NaN` entries dropped, and an 80/20 split (80% training set and 20% testing set) for this new data. Then complete the data type change below\n",
    "\n",
    "- `Weight_oz`,`Width`,`Height`\n",
    "- `Pub year` as type `int`; `NumPages` as type `int`; `Hard_or_Paper` as type `category`\n",
    "\n",
    "> To do this in a reproducible way, we're going to set a \"random seed\"; and, in preparation for this, let's take a moment to motivate our choice of $1985$ for the \"random seed\".\n",
    ">\n",
    "> Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss... Of course we might want to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now. At any rate, `NaN` entries can't be used in their raw form with the `scikit-learn` methodologies below, so we do need to remove them to proceed with our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54433c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9906b2d",
   "metadata": {},
   "source": [
    "### Q2: Tell ChatGPT that you are about to fit some `DecisionTreeClassifier` and ask what are the following preparation doing. Write a one to two sentence answer to this question in markdown cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(ab_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_noNaN[['NumPages', 'Thick', 'List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00028705",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eed1122",
   "metadata": {},
   "source": [
    "### Q3: Train a classification tree `clf` using only the  `List Price` variable to predict it is a hard cover book or a paper cover book (with  `max_depth` parameter set to `2`) .\n",
    "\n",
    "#### Use default values for all (tuning) parameters instantiating the Decision Tree Classifier.\n",
    "\n",
    "> - Hint 1: you'll need the `.fit()` method... ask Chat GPT about `DecisionTreeClassifier .fit()` would be helpful. For a deeper and more acurate explainatione see the `scikit-learn` \"Decision Trees\" documentation, which would be helpful...\n",
    "> - Hint 2: should you use the `train` data, or the `test` data, or all this data combined to fit the classification tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18aeaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6afe95b",
   "metadata": {},
   "source": [
    "Now you can visualize your tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff716801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c2d5e0",
   "metadata": {},
   "source": [
    "Or to make it more immediately readible we can use graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f664d469",
   "metadata": {},
   "source": [
    "And to limit the visualization itself we can add the `max_depth` parameter to our call of `export_graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a857671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c225c340",
   "metadata": {},
   "source": [
    "### Q4: How many observation are in the training data set and the test data set?\n",
    "\n",
    "> - Hint: a single observation consists of all the measurements made on a single entity. In Machine Learning, the  \"vector\" of all values measured for a single entity comprise a single \"observation\" so this just corresponds (typically) to a row of a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96a836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b3ce1f",
   "metadata": {},
   "source": [
    "### Q5: Why did you fit the classification tree with the data set you did?\n",
    "\n",
    "#### Write a one to two sentence answer to this question in markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce0805",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "890b5097",
   "metadata": {},
   "source": [
    "### Q6: Train classification tree `clf2` with features `NumPages`, `Thick` and `List Price` (to again predict if a book has hardback or paperback), use `GridSearchCV` to find best `max_depth`.\n",
    "\n",
    "#### Use the same train/test split data used so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfade813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d889386a",
   "metadata": {},
   "source": [
    "### Q7: Use the testing dataset you created in Q1 to create confusion matrices for `clf` and `clf2`. Report the sensitivity (true positive rate), specificity (true negative rate) and accuracy for each of the trees/models.\n",
    "\n",
    "#### Provide your answers as decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), and treat “Good” life expectancy as the positive response and prediction class. \n",
    "\n",
    "> Here are few things that might be helpful to ask ChatGPT: \n",
    "> - Hint 0: How can I use `np.round()`   \n",
    "> - Hint 1: Does the `y_true` or `y_pred` parameter go first in the `confusion_matrix` function?  \n",
    "> - Hint 2: How to label a confusion matrix in `sklearn`\n",
    "> - Hint 3: Confusion Matrices and Metrics explainations just below, you can always ask Chat GPT to explain it in detail or give you some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f395db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12218df9",
   "metadata": {},
   "source": [
    "<a id='cf'></a>\n",
    "# Confusion Matrices and Metrics\n",
    "\n",
    "- **Accuracy** is the proportion of cases that are correctly identified.\n",
    "- **Sensitivity** is the proportion of actual positive cases which are correctly identified to be positive (as true positives)\n",
    "    - **Sensitivity** is also known as **true positive rate (TPR)**\n",
    "- **Specificity** is the proportion of actual negative cases which are correctly identified to be negative (as true negative)\n",
    "    - **Specificity** is also known as **true negative rate (TNR)**\n",
    "- **False positive rates (FPR)** are defined to be the proportion of actually negative cases which are incorrectly identified (as false positives)\n",
    "- **False negative rates (FNR)** are defined to be the proportion of actually positive cases which are incorrectly identified (as false negatives)\n",
    "    - *but noticed how the FPR and FNR work in a sort of \"flipped\" manner in these definitions as they are defined with respect to the truth*\n",
    "\n",
    "In formulas\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy}  & = {} (TP+TN)/\\text{\"total # of cases\"}\\\\\n",
    "TPR & = {} TP/(TP+FN) = 1-FNR \\\\\n",
    "TNR & = {} TN/(TN+FP) = 1-FPR\n",
    "\\end{align*}\n",
    "\n",
    "and you can read more and see a (greatly expanded) handy list of formulas at the following [wikipedia page.](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72576c25",
   "metadata": {},
   "source": [
    "### Q8: Explain what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for `clf2` and `clf3`) are better\n",
    "\n",
    "#### Write a three to four sentence answer to this question in markdown cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(train.life_exp_good, clf.predict(train[['List Price']]), \n",
    "                                        labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix(test.life_exp_good, clf.predict(test[['List Price']]), \n",
    "                                        labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4eae0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526939c2",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Compared to understanding the contribution of different covariates towards the final predicted values of multiple linear regression models (where you can just read off the equation to see how predictions work), the extent to which we do not understand the overall contributions of the different features to the final predictions from our decision trees should feel a bit off-putting. To remedy this we can use so-called **Feature Importance** heuristics to judge how relatively important the different features are in the final decision tree predictions. \n",
    "\n",
    "\n",
    "### Q9: Ask ChatGPT or search on internet how to check feature Importance in `scikit-learn`, read the following paragraph and make sure you understand it \n",
    "\n",
    "> The way a decision tree is fit is that at each step in the construction process of adding a new decision node splitting rule to the current tree structure, all possible decision rules for all possible variables are considered, and the one that improves the prediction the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") legally and sufficiently according to the tuning parameters rules of the decision tree is added to the decision tree.  The overall \"criterion\" noted above improves with each new decision node splitting rule, so the improvement can thus be tracked and the contributions attributed to the feature upon which the decision node splitting rule is based.  This means the relative contribution of each feature to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dd793",
   "metadata": {},
   "source": [
    "### Q10: Which predictor variable is most important for making predictions according to `clf2`?\n",
    "\n",
    "#### Visualize the *Feature Importances* and report the name of most important feature and its numeric *Feature Importance* value\n",
    "\n",
    "> - Hint 0: `.feature_importances_` &`.feature_names_in_` \n",
    "> - Hint 1: Visualize Feature Importances : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fab485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10_most_important_feature = \n",
    "Q10_most_important_feature_percentage_score = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d92c",
   "metadata": {},
   "source": [
    "### Q11: Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees.\n",
    "\n",
    "#### Write a couple sentences or so in markdown cell below to answer this question \n",
    "\n",
    "> Hint: linear model regression predicts continuous real-valued averages for a given configuration of covariate values (or, feature values, if we're using machine learning terminology instead of statistical terminology), whereas a binary classification model such as a binary classification tree predicts 0/1 (\"yes\" or \"no\") outcomes (and gives the probability of a 1 \"yes\" (or \"success\") outcome from which a 1/0 \"yes\"/\"no\" prediction can be made; but, this is not what is being asked here. This question is asking \"what's the difference in the way we can interpret and understand how the predictor variables influence the predictions in linear model regression based on the coefficients versus in binary decision trees based on the Feature Importances?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366156c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment questions would go here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
