{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8969b8",
   "metadata": {},
   "source": [
    "## Demo [35 minutes]\n",
    "1. Demonstrate accessing the [Course GitHub Repo](https://github.com/pointOfive/STA130_ChatGPT) from Quercus and opening a notebook in [UofT Jupyterhub](https://datatools.utoronto.ca) [or google collab](https://colab.research.google.com/)\n",
    "\n",
    "\n",
    "2. Demonstrate Jupyter Notebooks as a \"python calculator\"\n",
    "\n",
    "\n",
    "3. Demonstrate Jupyter Notebooks as a [\"Markdown editor\"](http://markdownguide.org)\n",
    "\n",
    "\n",
    "4. Demonstrate using [Copilot](https://copilot.microsoft.com/) [or [ChatGPT](https://chat.openai.com/)] for \n",
    "    1. finding an amusing, funny, or otherwise interesting data set which has missing values and is available online through a url link to a csv file, and then\n",
    "    2. loading the data into the notebook with pandas and \"analyzing the missing data\" (by finding out how much missing data there is in each column of the data set) \n",
    "> This can either [go well](../CHATLOG/SLS/COP/00006_copilot_funnyamusingNAdataset.md) or [go poorly](../CHATLOG/SLS/COP/00002_copilot_funnyamusingNAdataset.md), but a single demonstration of either is sufficiently informative... students will experience the varied nature of these interactions first-hand themselves throughout the semester\n",
    "> - The positive example above used \"creative\" mode while the poor example above used \"precise\" mode; however, this likely has as much to do with the nature of the prompting and engagement as the actual type of bot being used... for example, [here](../CHATLOG/SLS/COP/00007_copilot_funnyamusingNAdatasetV4.md) \"precise\" mode worked well...\n",
    "> - ChatGPT3.5 can similarly result in \n",
    "> [productive (useful)](../CHATLOG/SLS/GPT/00001_gpt3p5_villagersdata.md)\n",
    "> and [unproductive (frustrating and annoying)](../CHATLOG/SLS/GPT/00002_gpt3p5_funnyasusingNAdataset.md) sessions\n",
    ">\n",
    "> The objective here is to see if the ChatBot can fullfil the parameters of the request, so the expercise is to confirm that the requirements (of working url links, a \"funny or amusing\" nature, and the presence of missingness) are met\n",
    "> - **Topics of \"handling/imputing/filling/assumptions regarding missing values\" are \"out of scope\" for STA130 and should be noted as such** \n",
    "> - You can experiement with either adding all the desired critia in the first prompt, or iteratively updating the ChatBot interaction towards `import` and `pandas`, loading the data into your notebook from a url link to a csv file, and assessing missing values in a data set... *students should begin considering how the nature of the prompting can influence the trajectory of the interaction*\n",
    "> - The examples above primed the ChatBots to respond with a online url (as opposed to a local file) and `pandas` code (although ChatBots often respond using `python` since it's such a prevalent and ubiquitous programming language\n",
    "> \n",
    "> ChatBots at this point do not seem to be a substitue for reviewing data yourself at a data set repository such as [TidyTuesday](https://github.com/rfordatascience/tidytuesday), or other data repositiory resources the interaction might raise or suggest  \n",
    "> - ChatBots don't seem to know much about what's in the data sets they suggest, as they do not always provide working url links, often forget about specific aspects of a request; but, ChatBot interactions nonetheless can provide help with brainstorm ideas and provide a way to \"search for content\" (even within a specific website)\n",
    "\n",
    "\n",
    "5. Demonstrate prompting the ChatBot to \"Please provide a summary of our interaction for submitting as part of the requirements of a assignment\" and to \"Please provide me with the final working verson of the code that we created\" \n",
    "> Share this with the students through a piazza post and a Quercus announcement so they can use this later for their homework assignment if they wish\n",
    "\n",
    "\n",
    "6. Demonstrate saving your python jupyter notebook in your own account and \"repo\" on [github.com](github.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379538b1",
   "metadata": {},
   "source": [
    "## Communication [65 minutes]\n",
    "        \n",
    "1. In 8 groups of 3, or thereabouts...\n",
    "    1. **[About 15 of the 65 minutes]** Ice breakers / introductions\n",
    "        1. Each person may bring two emojis to a desert island... reveal your emojis at the same time... for emojis selected more than once the group should select one additional emoji\n",
    "        2. Where are you from, what do you think your major might be, and what's an \"interesting\" fact that you're willing to share about yourself?\n",
    "        \n",
    "    2. **[About 10 of the 65 minutes]** These are where all the bullet holes were in the planes that returned home so far after some missions in World War II\n",
    "        1. Where would you like to add armour to planes for future missions?\n",
    "        2. Hint: there is a hypothetical data set of the bullet holes on the planes that didn't return which is what we'd ideally compare the data set we observe to...\n",
    "        \n",
    "           ![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Survivorship-bias.svg/640px-Survivorship-bias.svg.png)\n",
    "           \n",
    "    3. **[About 10 of the 65 minutes]** Monte Hall problem: there are three doors, one of which has a prize, and you select one and the gameshow host reveal one of the other two doors to not have the prize... would you like to change your guess to the other unknown door?\n",
    "\n",
    "       ![](https://mathematicalmysteries.org/wp-content/uploads/2021/12/04615-0sxvwbnzvvnhuklug.png) \n",
    "    \n",
    "4. **[About 30 of the 65 minutes]** Review the experience of the groups for the WW2 planes and Monte Hall problems\n",
    "    1. For each of these problems, have students vote on whether their groups (a) agreed on answers from the beginning, (b) agreed only after some discussion and convincing, or (c) retained somewhat divided in their opinions of the best way to proceed\n",
    "    2. Briefely discuss a few of the general answers the groups arrived at\n",
    "    3. Prompt a [Copilot](https://copilot.microsoft.com/) [or [ChatGPT](https://chat.openai.com/)] ChatBot to introduce and explain \"survivorship bias\" using spotify songs as an example (like [this](../CHATLOG/SLS/COP/00009_copilot_survivorshipbias_spotify.md) or [this](../CHATLOG/SLS/GPT/00003_gpt3p5_spotify_Survivorship_Bias.md) or you could instead try to approach things more generally like [this](../CHATLOG/SLS/COP/00008_copilot_survivorshipbiasgeneral.md) or [this](../CHATLOG/SLS/GPT/00004_gpt3p5_general_Survivorship_Bias.md)) and see if students are able to generalize this idea for the WW2 planes problem and if they find it to be a convincing argument to understand the problem\n",
    "    4. Prompt a [Copilot](https://copilot.microsoft.com/) [or [ChatGPT](https://chat.openai.com/)] ChatBot to introduce and explain the Monte Hall problem and see if the students find it understandable and convincing\n",
    "        1. Very briefly show the following transcript logs where the ChatBot fails when it's pushed to explain the problem using a formal probabilistic argument... [ChatGPT3.5 fails by wrongly calculating a probability of 1/2](../CHATLOG/SLS/GPT/00005_gpt3p5_MonteHallWrong.md), while [Copilot on \"creative\" mode honestly doesn't fair much better without substantial guidance...](../CHATLOG/SLS/COP/00010_copilot_montehallwrong.md )... \n",
    "            1. **Both of these examples go to show the that there are clear limits to how deeply ChatBots are able to \"reason\" which of course is a result of the fact that the information they base their regurgitations can lead to a \"garbage in, garbage out\" situation...**\n",
    "\n",
    "        2. Discuss how the nature of prompting and follow up questions influences the nature of the conversation and the degree to which the discussion can be directed and extended "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad910f",
   "metadata": {},
   "source": [
    "## Homework [0 minutes]\n",
    "\n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading assignment. Indeed, in the most general terms, STA130 Homework essentially boils down to providing demonstrations of reading comprehension. However, unlike reading a textbook, STA130 Homework is meant to be a dynamic and interactive experience that allows you to follow up and pursue open questions that remain unclear. And doing so often requires some degree of effective two-way communication beyond copy-pasting question prompts.  All together, this makes the homework a more realistic communication activity practicing fundamental communication skills through the process of building conceptual understanding, and hence a very valuable exercise.\n",
    "\n",
    "As you proceed, it will likely become tempting to increasingly rely on the ChatBots you interact with \"do the work for you\"; but, if you find yourself frustrated with their inability to give you the answers you're looking for, you're doing the STA130 Homework the wrong way. It's your responsibility to use the ChatBots to help you build your understanding so you can leverage them (and other resources) effectively moving forward, not the ChatBots responsibility to give you the anwers.\n",
    "\n",
    "\n",
    "> Code and write all your answers (for both the \"Prelecture\" and \"Postlecture\" HW) in a python notebook (in code and markdown cells) and paste summaries of your ChatBot sessions (including log link(s) if you're using ChatGPT) within your notebook.\n",
    "> - You can create summaries of your ChatBot sessions by using concluding prompts such as, \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\", and, \"Please provide me with the final working verson of the code that we created together\".\n",
    "> - Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking. \n",
    "> \n",
    "> *The marking rubic (which may award partial credit) is as follows:*\n",
    "> - [0.1 points]: All relevant ChatBot summaries (including log link(s) if you're using ChatGPT) are reported within the notebook\n",
    "> - [0.2 points]: Reasonable general definitions for \"2.B\"\n",
    "> - [0.2 points]: Demonstrated understanding regarding \"4\"\n",
    "> - [0.2 points]: A sensible justification for the choice in \"7.D\"\n",
    "> - [0.3 points]: Requested assessment of ChatGPT and google in \"8\"\n",
    "\n",
    "#### Prompt Engineering? \n",
    "\n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate conversations which an be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed arise from ChatBots (a) going beyond the scope of materials intended to be addressed by the question, and (b) unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. In the case of the former (a), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (b), is often the result of initial prompting that leads to poor developments in navigating the material, and these are likely best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)... the thing is, you'll like it better if you do it to copilog before copilot does it to you ;)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "\n",
    "### Prelecture HW\n",
    "\n",
    "1. Pick one of the data sets from your **TUT demo** (or one of the many example demos linked above or your own version of the ChatBot session if you wish) and use the code identified from the interaction to import the data confirm that the data set has missing values\n",
    "\n",
    "```python\n",
    "# For example, based on the \"example demo\" you might use\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.isnull().sum()) \n",
    "```\n",
    "\n",
    "2. Start a new ChatBot session with an initial prompt introducing to the data set you're using and a request for help analyzing the data... *be ready to limit the scope of the responses of the ChatBot if it's responding with more information than you need to answer the sub questions below.* \n",
    "    > Something like \"I've downloaded a data set about characters from animal crossings (from https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv), and I'd like to get your help analyzing some things about this data\" would be a good initial prompt to start with; but, you might need to quickly follow this up with \"I've downloaded the data and want to understand the size (or dimensions) of the data set to start with.\"\n",
    "    > - Some ChatBots (like ChatGPT) will try suggesting that you to upload your data, but if you do so [they will likely end your session without providing any analysis unless you purchase an upgrade](../CHATLOG/SLS/GPT/00006_gpt3p6_LoadDataPaywall.md); so, if you encounter this, instead of uploading yoru data just respond with the follow up message suggested right up above.\n",
    "    > > For this class we don't want ChatBots to just do the analysis for us. We instead want ChatBots to help us understand the steps we need to take to analyze the data. Therefore, **you DO NOT need to purchase an upgraded version of any ChatBots**. \n",
    "    > > - [Copilot Pro](https://copilot.microsoft.com/) which you have access to through your UofT account is anyway based on [ChatGPT4.0](https://chat.openai.com/)...\n",
    "\n",
    "    1. Use code provided in your ChatBot session in response to the prompts suggested above to print out the number of rows and columns of the data set \n",
    "    2. Write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your data set  \n",
    "\n",
    "\n",
    "3. Ask the ChatBot how to go about summarizing the data set and use the suggested code to provide these summaries for your data set... *again be ready to limit the scope of the responses of the ChatBot if it's responding with more information than you need to answer the sub question below.*\n",
    "    > ChatBots will likely provide you with a wide variety of approaches and techniques for summarizing your data set, so re-prompting the ChatBot with something like \"What's the simplest form of summarization of this data set that I could do and how do I do it in Python?\", and/or suggesting the specific summarization methods requested below will be helpful \n",
    "    > - Hint 1: consider dividing the code that ChatBot provides you into different jupyter notebook cells so that each cell concludes with a key printed result\n",
    "    > - Hint 2: the last line of code in a jupyter notebook cell will automatically print out in a formatted manner, so replacing something like `print(df.head())` with `df.head()` as the last line of a cell provides a sensible way to organize your code\n",
    "    > - Hint 3: jupyter notebook printouts usaully don't show all of the data (when there's too much to show, like if `df.describe()` include results for many columns), but the printouts just show enough of the data to give an idea of what the results are which is all we're looking for at the moment\n",
    "\n",
    "    1. Use your ChatBot session to help you create working examples of using  `df.describe()` and `df['column'].value_counts()` on your data\n",
    "\n",
    "                \n",
    "4. If the data set you're using has missing values in numeric variables, explain (perhaps using help from the ChatBot session if needed) the discrepancies between size of the data set given by `df.shape` and what is reported by `df.describe()` with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column\n",
    "    > If the data set you're using does not have missing values in numeric variables (e.g., the `\"villagers.csv\"` example above has only a single numeric variable `row_n` which has no missing values), instead download and use the \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\" data to answer this question  \n",
    "    > - Hint 1: In (a) above, the \"columns it analyzes\" are the columns of the output of `df.describe()` which will only include \"numeric\" columns by default, but you can can see all the columns in a data set using `df.columns`\n",
    "    > - Hint 2: Make sure `df.shape` is refering to the data set you think it is... if you've loaded a different data set it might have a different name now!\n",
    "    >\n",
    "    > #### If you get any errors, copy and paste them as a response to the ChatBot, and see if it can help you resove them by adding the suggested adjustments to your code and reruning all your code.\n",
    "    \n",
    "    \n",
    "5. Use your ChatBot session to help understand the difference between an \"attribute\" (such as `df.shape` which does not end with `()`) and a \"method\" (such as `df.describe()` which does end with `()`) and provide your own paraphrasing summarization of that difference\n",
    "\n",
    "    > - Hint 1: The fact that a \"method\" (such as `df.describe()`) ends with `()` suggests that \"methods\" are essentially something that we would call a \"function\" in programming language terminology\n",
    "    > - Hint 2: Without getting too technical or \"in the weeds\", how might we describe what the difference is between what a \"function\" is in a programming language versus what a \"function\" is in mathematics? \n",
    "\n",
    "\n",
    "6. Besides 'count', `df.describe()` provides the 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Define (perhaps using help from the ChatBot session if needed) the meaning of each of these summary statistics \n",
    "    > Hint: the answers here should make it obvious why these can only be calculated for numeric variables in a data set, which should help explain the answer to \"4\" above...\n",
    "    \n",
    "\n",
    "7. Missing data can be considered \"across rows\" or \"down columns\".  Consider how `df.dropna()` or `del df['col']` should be applied to most efficiently use the available data, and answer the following sub questions briefly in your own words.\n",
    "\n",
    "    > Start a new ChatBot session **[but remember to first ask for summaries of your current session and perhaps coding results (so you can supply these in the homework as requested)]**, since your last session has likely gotten quite long and has covered a lot of material at this point.  It can sometimes be helpful to reset ChatBot sessions to refocus them on the topics of inquiry without too much backlog history that might push things in certain unnecessary directions. Of course, you can always re-introduce material from earlier conversations as it's relevant, such as for answering \"D\" below based on reintroducing and updating code you made in a previous ChatBot session.  \n",
    "    > - More sophisticated analyses for \"filling in\" rather than removing missing data (as considered here) are possible (based on making assumptions about missing data and using specific imputation methods or models) but these are \"beyond the scope\" of STA130 so this can be safely ignored for now\n",
    "    > - This question is not interested in the general benefits of imputing missing data, or the general benefits of using `df.dropna()` and/or `del df['col']` to remove missing data, just how to most efficiently remove missing data if a user chooses to do so\n",
    "    >\n",
    "    > **A key issue to be aware of when asking ChatBots for help with something is that they are not running and checking code for correctess, and they often intertwine written instructions with code instructions. BEFORE YOU RUN ANY CODE provided by a ChatBot, you should check the following:**\n",
    "    >\n",
    "    > 1. If this code changes an object or data, are you sure you want to run this code right away?\n",
    "    > 2. Should you perhaps make a copy of the current object or data in case you want to \"undo\" actions of code (e.g., by `df_saved=df.copy()` or simply reloading the data)?\n",
    "    > 3. Is the code unaware of changes to the object or data its changing and assuming it's being applied to some other (such as an original unadjusted) version of the object or data? \n",
    "    >\n",
    "    > #### If you get any errors, copy and paste them as a response to the ChatBot, and see if it can help you resove them by adding the suggested adjustments to your code and reruning all your code.\n",
    "\n",
    "    1. Provide an example of a \"use case\" in which using `df.dropna()` might be peferred over using `del df['col']`\n",
    "    2. Provide an example of \"the opposite use case\" in which using `del df['col']` might be preferred over using `df.dropna()` \n",
    "    3. Discuss why applying `del df['col']` before `df.dropna()` when both are used together could be important\n",
    "    4. Remove all missing data from one of the data sets you're considering using some combination of `del df['col']` and/or `df.dropna()` and give a justification for your approach, including a \"before and after\" report of the results of your approach for your data set. \n",
    "\n",
    "\n",
    "8. When we first used `df.describe()` above we had not yet explicitly removed the missing values from the data set in the manner of the previous question above, but `df.describe()` seemed to work and provide results anyway... Assuming we're not going to fill in any missing data values using any missing data imputation methods, what's the one best argument for not explicitly removing missing values from a data set? And alternatively, what's the one best argument for nonetheless explicitly removing missing values from a data set? Give a simple summary in your own words explaining what you think the best arguments here are.\n",
    "\n",
    "    > - Hint 1: are you clear about what `df.describe()` does with the data in each columns it analyzes if there is missing data in the column in question? \n",
    "    > - Hint 2: are you sure all methods that we might want to apply to data will be able to handle missing data the way `df.describe()` does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7d86644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "After cleaning:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Initial state of missing data\n",
    "initial_missing_data = titanic_df.isnull().sum()\n",
    "print(\"Before cleaning:\")\n",
    "print(initial_missing_data)\n",
    "\n",
    "# Remove columns with missing data that are not important for the analysis\n",
    "columns_to_remove = ['age', 'deck']\n",
    "for col in columns_to_remove:\n",
    "    if col in titanic_df.columns:\n",
    "        del titanic_df[col]\n",
    "\n",
    "# Remove any rows with missing 'embark_town' data\n",
    "titanic_df.dropna(subset=['embark_town'], inplace=True)\n",
    "\n",
    "# Final state of missing data\n",
    "final_missing_data = titanic_df.isnull().sum()\n",
    "print(\"After cleaning:\")\n",
    "print(final_missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "203da43f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: Fare'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/hnfh4zsn34d7xpbz9226pz200000gn/T/ipykernel_17841/178043127.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Group the data by 'Pclass' and describe the 'Fare' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass_fare_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pclass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Fare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_fare_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/PyMC/lib/python3.11/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1769\u001b[0m                 \u001b[0;34m\"Use a list instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m             )\n\u001b[0;32m-> 1771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/PyMC/lib/python3.11/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column not found: {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: Fare'"
     ]
    }
   ],
   "source": [
    "# Group the data by 'Pclass' and describe the 'Fare' column\n",
    "class_fare_summary = titanic_df.groupby('pclass')['Fare'].describe()\n",
    "print(class_fare_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f9eda9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'sex', 'sibsp', 'parch', 'fare', 'embarked',\n",
      "       'class', 'who', 'adult_male', 'embark_town', 'alive', 'alone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(titanic_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "378b8e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std  min       25%      50%   75%       max\n",
      "pclass                                                                     \n",
      "1       214.0  84.193516  78.746457  0.0  30.77185  58.6896  93.5  512.3292\n",
      "2       184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "3       491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n"
     ]
    }
   ],
   "source": [
    "# Group the data by 'pclass' and describe the 'fare' column\n",
    "class_fare_summary = titanic_df.groupby('pclass')['fare'].describe()\n",
    "print(class_fare_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d258e0",
   "metadata": {},
   "source": [
    "### Postlecture HW\n",
    "\n",
    "9. This problem will guide you through exploring how to use a ChatBot to troubleshoot code using the \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\" data set. Give brief explanations in your own words for any requested answers to the questions below. \n",
    "    > To initialially constrain the scope of the reponses from your ChatChat, start a new ChatBot session with the following slight variation on the initial prompting approach from \"2\" above\n",
    "    > - \"I am going to do some initial simple summary analyses on the titanic data set I've downloaded (https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv) which has some missing values, and I'd like to get your help understanding the code I'm using and the analysis it's performing\"\n",
    "    > \n",
    "    > > Remember, ChatGPT will allow you to upload your data, but for this class we don't want ChatGPT to just do the analysis for us. **DO NOT purchase an upgrade of ChatGPT**. We instead want ChatBots to help us understand the steps we need to take to analyze the data. \n",
    "    > > - [Copilot Pro](https://copilot.microsoft.com/) which you have access to through your UofT account is anyway based on [ChatGPT4.0](https://chat.openai.com/)...   \n",
    "    \n",
    "    1. Use your ChatBot session to understand what `df.groupby(\"col1\")[\"col2\"].describe()` does and then demonstrate and explain this using a different example from your data set other than what the ChatBot automatically provide for you\n",
    "        > If needed, you can help guide the ChatBot by showing it the code you've used to download the data **AND provide it with the names of the columns** using either a summary of the data with `df.describe()` or just `df.columns` as demonstrated [here](../CHATLOG/COP/00017_copilot_groupby.md)\n",
    "    \n",
    "    2. Assuming you've not yet removed missing values in the manner of question \"8\" above, `df.describe()` would have different values in the `count` value for different data columns depending on the missingness present in the original data.  Why do these capture something fundamentally different from the values in the `count` that result from doing something like `df.groupby(\"col1\")[\"col2\"].describe()`?\n",
    "\n",
    "        > - Questions \"4\" and \"6\" and \"8\" above address how missing values are handled by `df.describe()` (which is reflected in the `count` output of this method); but, `count` in conjunction with `group_by` has another primary function that's more important than addressing missing values (although missing data could still play a role here).\n",
    "\n",
    "    3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors \n",
    "        > First share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT\n",
    "    \n",
    "        1. Forget to include `import pandas as pd` in your code \n",
    "            > Use Kernel->Restart from the notebook menu to restart the jupyter notebook session unload imported libraries and start over so you can create this error\n",
    "            >  - When python has an error, it sometimes provides a lot of \"stack trace\" output, but that's not usually very important for troubleshooting. For this problem for example, all you need to share with ChatGPT or search on google is `\"NameError: name 'pd' is not defined\"`\n",
    "\n",
    "        2. Mistype \"titanic.csv\" as \"titanics.csv\" \n",
    "            1. If ChatBot troubleshooting is based on downloading the file, just replace the whole url with \"titanics.csv\" and try to troubleshoot the subsequent `FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv'` (assuming the file is indeed not present)\n",
    "            2. Explore introducing typos into a couple other parts of the url and note the slightly different errors this produces\n",
    "      \n",
    "        3. Try to use a dataframe before it's been assigned into the variable\n",
    "            > You can simulate this by just misnaming the variable. For example, if you should write `df.group_by(\"col1\")[\"col2\"].describe()` based on how you loaded the data, then instead write `DF.group_by(\"col1\")[\"col2\"].describe()`\n",
    "            > - Make sure you've fixed your file name so that's not the error any more\n",
    "        \n",
    "        4. Forget one of the parenthesis somewhere the code\n",
    "            > For example, if the code should be `pd.read_csv(url)` the change it to `pd.read_csv(url` \n",
    "        \n",
    "        5. Mistype one of the names of the chained functions with the code \n",
    "            > For example, try something like `df.group_by(\"col1\")[\"col2\"].describe()` and `df.groupby(\"col1\")[\"col2\"].describle()`\n",
    "        \n",
    "        6. Use a column name that's not in your data for the `groupby` and column selection \n",
    "            > For example, try capitalizing the columns for example replacing \"sex\" with \"Sex\" in `titanic_df.groupby(\"sex\")[\"age\"].describe()`, and then instead introducing the same error of \"age\"\n",
    "        \n",
    "        7. Forget to put the column name as a string in quotes for the `groupby` and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question\n",
    "            > For example, something like `titanic_df.groupby(sex)[\"age\"].describe()`, and then `titanic_df.groupby(\"sex\")[age].describe()`\n",
    "    \n",
    "\n",
    "10. Have you interacted with a ChatBot to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it? \n",
    "> Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca7d22",
   "metadata": {},
   "source": [
    "#### Afterward\n",
    "\n",
    "Here are few ideas of some other kinds of interactions you might consider exploring with ChatGPT...\n",
    "\n",
    "> While these are likely to be extremely practically valuable, they are not a part of the homework assignment, so do not include anything related to these in your homework submission\n",
    "\n",
    "- With respect to improving ones ability in statistics, coding, communication, and other key data science skills\n",
    "    - what is ChatGPTs perception its own capabilities and uses as an AI-driven assistance tool \n",
    "    - and does ChatGPTs assessment of itself influence or agree with your own evalution of ChatGPT? \n",
    "\n",
    "- ChatGPT can introduce and explain the \"World War 2 planes\" problem and the \"Monte Hall\" problem... \n",
    "    - how well does it seem to do and showing and explain other \"unintuitive surprising statistics paradoxes\"?\n",
    "\n",
    "- If you consider the process of writing about why you chose to take this course, and the skills you were hoping to build through this course with respect to your current ideas about what possible careers \n",
    "    - and how do you think the exercise would be different if you framed it as a dialogue with ChatGPT\n",
    "    - and do you think the difference could be positive and productive, or potentially biasing and distracting?\n",
    "    \n",
    "- ChatGPT sometimes immediately responds in simple helpful ways, but other times it gives a lot of information that can be overwheling... are you able to prompt and interact with ChatGPT in manner that keeps its reponses helpful and focused on what you're interested in? \n",
    "\n",
    "- ChatGPT tends to respond in a fairly empathetic and supportive tone...\n",
    "    - do you find it helpful to discuss concerns you might have about succeeding in the course (or entering university more generally) with ChatGPT?\n",
    "    \n",
    "- For what purposes and in what contexts do you think ChatGPT could provide suggestions or feedback about your experiences that might be useful? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
