{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple Linear Regression (SLR) is a technique used to model the relationship between one dependent variable (Y) and one independent variable (X). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ff7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple Linear Regression (SLR) is a statistical method used to model the relationship between a dependent variable (Y) and an independent variable (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cba110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels\n",
    "model = smf.ols(formula='Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the generated data and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color='blue', label='Generated Data')\n",
    "plt.plot(data[\"X\"], model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "plt.plot(data[\"X\"], beta_0 + beta_1 * X, color='red', linestyle='--', label='True Regression Line (from Question 1)')\n",
    "plt.title('Fitted Simple Linear Regression Model with True Line Overlay')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Explanation of the two lines:\n",
    "# The plot now includes two lines:\n",
    "# 1. **True Regression Line (Red Dashed Line)**: This line represents the theoretical relationship defined by `beta_0` and `beta_1` used to generate the data. It shows the true underlying relationship without the influence of random error.\n",
    "# 2. **Fitted Regression Line (Green Line)**: This line is the result of fitting the model to the simulated data. It represents the estimated relationship based on the sample data and can vary due to random sampling variation.\n",
    "# The difference between the two lines highlights the effect of random sampling variation: while the true line shows the expected relationship, the fitted line accounts for the noise introduced by the error term and reflects sample-specific estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The fitted_model.fittedvalues in the context of the statsmodels Simple Linear Regression model are derived using the estimated parameters of the model, specifically fitted_model.params. These parameters include the estimated intercept (β₀) and slope (β₁).\n",
    "\n",
    "How fitted_model.fittedvalues are Derived:\n",
    "The fitted_model.params attribute holds the estimated coefficients for the regression model, which are computed based on the sample data during model fitting.\n",
    "The fittedvalues are calculated by applying the estimated regression equation to the predictor values X\n",
    "\n",
    "X represents the independent variable data.\n",
    "Relationship to fitted_model.summary().tables[1]:\n",
    "The fitted_model.summary().tables[1] displays the parameter estimates (\\hat{\\beta}_0 and \\hat{\\beta}_1), their standard errors, t-values, and p-values. These estimates are used to form the fittedvalues by substituting them into the linear regression equation.\n",
    "fitted_model.params.values directly gives you an array of the parameter values: [intercept, slope]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08058c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The line chosen for the fitted model using the \"ordinary least squares\" (OLS) method is the one that minimizes the sum of the squared differences between the observed data points and the predicted values from the line. This line is often called the \"best fit\" line.\n",
    "\n",
    "Why \"Squares\" are Used:\n",
    "The reason OLS uses the squares of the differences (residuals) is to ensure that all deviations contribute positively to the total error and to give larger deviations disproportionately more weight. Squaring emphasizes larger errors, making the model more sensitive to outliers. This approach helps identify the line that results in the smallest overall error when predicting Y from X.\n",
    "\n",
    "In essence, OLS chooses the line where the sum of squared residuals is minimized, providing a balanced fit that reduces the impact of random variations in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf012b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The first expression being referred to, fitted_model.rsquared, is known as the coefficient of determination or R-squared. Here's an explanation of why it represents \"the proportion of variation in (outcome) Y explained by the model\" and its interpretation in the context of Simple Linear Regression。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22577b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "To evaluate the compatibility of the Simple Linear Regression model assumptions with a dataset, we must consider several assumptions integral to SLR. Here are a couple of key assumptions that may not align with certain data characteristics:\n",
    "\n",
    "1. Linearity Assumption:\n",
    "The SLR model assumes a linear relationship between the independent variable X and the dependent variable Y. If the data suggests a non-linear pattern (e.g., a curve or other complex relationship), the linear model is not appropriate.\n",
    "\n",
    "2. Homoscedasticity (Constant Variance of Errors):\n",
    "SLR assumes that the variance of the residuals (errors) is constant across all levels of X. If the spread of the residuals increases or decreases as X changes (e.g., forming a funnel shape), this violates the homoscedasticity assumption.\n",
    "\n",
    "Checking Compatibility with Example Data:\n",
    "Visual Inspection for Linearity: Plotting Y versus X and observing if the data follows a roughly straight line helps identify non-linearity.\n",
    "Residual Plot for Homoscedasticity: Plotting residuals against fitted values can reveal if the variance of errors is consistent. A pattern or change in spread suggests heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a410b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels\n",
    "model = smf.ols(formula='Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the generated data and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color='blue', label='Generated Data')\n",
    "plt.plot(data[\"X\"], model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "plt.plot(data[\"X\"], beta_0 + beta_1 * X, color='red', linestyle='--', label='True Regression Line (from Question 1)')\n",
    "plt.title('Fitted Simple Linear Regression Model with True Line Overlay')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Explanation of the two lines:\n",
    "# The plot now includes two lines:\n",
    "# 1. **True Regression Line (Red Dashed Line)**: This line represents the theoretical relationship defined by `beta_0` and `beta_1` used to generate the data. It shows the true underlying relationship without the influence of random error.\n",
    "# 2. **Fitted Regression Line (Green Line)**: This line is the result of fitting the model to the simulated data. It represents the estimated relationship based on the sample data and can vary due to random sampling variation.\n",
    "# The difference between the two lines highlights the effect of random sampling variation: while the true line shows the expected relationship, the fitted line accounts for the noise introduced by the error term and reflects sample-specific estimates.\n",
    "\n",
    "# Null hypothesis for Simple Linear Regression:\n",
    "# H0: beta_1 = 0 (There is no linear association between X and Y)\n",
    "# This hypothesis states that the slope of the regression line is zero, implying that changes in X do not predict changes in Y.\n",
    "\n",
    "# Using the fitted model to assess the evidence against H0:\n",
    "# The p-value associated with the slope coefficient (beta_1) from model.summary().tables[1] indicates the strength of the evidence.\n",
    "# If the p-value is below a certain threshold (e.g., 0.05), we reject H0, suggesting there is a statistically significant linear relationship.\n",
    "\n",
    "# Interpretation for the Old Faithful Geyser dataset:\n",
    "# The analysis involves fitting a Simple Linear Regression model to the geyser data and checking if the p-value for beta_1 is low enough to reject the null hypothesis.\n",
    "# A low p-value suggests that the duration of eruptions (predictor) and the waiting time (outcome) have a significant linear relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels\n",
    "model = smf.ols(formula='Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the generated data and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color='blue', label='Generated Data')\n",
    "plt.plot(data[\"X\"], model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "plt.plot(data[\"X\"], beta_0 + beta_1 * X, color='red', linestyle='--', label='True Regression Line (from Question 1)')\n",
    "plt.title('Fitted Simple Linear Regression Model with True Line Overlay')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Explanation of the two lines:\n",
    "# The plot now includes two lines:\n",
    "# 1. **True Regression Line (Red Dashed Line)**: This line represents the theoretical relationship defined by `beta_0` and `beta_1` used to generate the data. It shows the true underlying relationship without the influence of random error.\n",
    "# 2. **Fitted Regression Line (Green Line)**: This line is the result of fitting the model to the simulated data. It represents the estimated relationship based on the sample data and can vary due to random sampling variation.\n",
    "# The difference between the two lines highlights the effect of random sampling variation: while the true line shows the expected relationship, the fitted line accounts for the noise introduced by the error term and reflects sample-specific estimates.\n",
    "\n",
    "# Null hypothesis for Simple Linear Regression:\n",
    "# H0: beta_1 = 0 (There is no linear association between X and Y)\n",
    "# This hypothesis states that the slope of the regression line is zero, implying that changes in X do not predict changes in Y.\n",
    "\n",
    "# Using the fitted model to assess the evidence against H0:\n",
    "# The p-value associated with the slope coefficient (beta_1) from model.summary().tables[1] indicates the strength of the evidence.\n",
    "# If the p-value is below a certain threshold (e.g., 0.05), we reject H0, suggesting there is a statistically significant linear relationship.\n",
    "\n",
    "# Interpretation for the Old Faithful Geyser dataset:\n",
    "# The analysis involves fitting a Simple Linear Regression model to the geyser data and checking if the p-value for beta_1 is low enough to reject the null hypothesis.\n",
    "# A low p-value suggests that the duration of eruptions (predictor) and the waiting time (outcome) have a significant linear relationship.\n",
    "\n",
    "# Restricting the dataset to short wait times and evaluating evidence against the null hypothesis:\n",
    "short_wait_limits = [62, 64, 66]\n",
    "for limit in short_wait_limits:\n",
    "    short_data = data[data[\"X\"] < limit]\n",
    "    short_model = smf.ols(formula='Y ~ X', data=short_data).fit()\n",
    "    print(f\"\\nSummary for wait times less than {limit} minutes:\")\n",
    "    print(short_model.summary())\n",
    "\n",
    "    # Plot the restricted data and fitted regression line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(short_data[\"X\"], short_data[\"Y\"], color='blue', label=f'Data for wait < {limit} min')\n",
    "    plt.plot(short_data[\"X\"], short_model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "    plt.title(f'Fitted Simple Linear Regression Model for Wait < {limit} min')\n",
    "    plt.xlabel('Predictor Variable (X)')\n",
    "    plt.ylabel('Outcome Variable (Y)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check the p-value for beta_1 to assess evidence against H0\n",
    "    p_value = short_model.pvalues['X']\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Evidence suggests a significant linear relationship for wait < {limit} min (p-value: {p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"No significant linear relationship detected for wait < {limit} min (p-value: {p_value:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1121d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels\n",
    "model = smf.ols(formula='Y ~ X', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the generated data and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[\"X\"], data[\"Y\"], color='blue', label='Generated Data')\n",
    "plt.plot(data[\"X\"], model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "plt.plot(data[\"X\"], beta_0 + beta_1 * X, color='red', linestyle='--', label='True Regression Line (from Question 1)')\n",
    "plt.title('Fitted Simple Linear Regression Model with True Line Overlay')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Explanation of the two lines:\n",
    "# The plot now includes two lines:\n",
    "# 1. **True Regression Line (Red Dashed Line)**: This line represents the theoretical relationship defined by `beta_0` and `beta_1` used to generate the data. It shows the true underlying relationship without the influence of random error.\n",
    "# 2. **Fitted Regression Line (Green Line)**: This line is the result of fitting the model to the simulated data. It represents the estimated relationship based on the sample data and can vary due to random sampling variation.\n",
    "# The difference between the two lines highlights the effect of random sampling variation: while the true line shows the expected relationship, the fitted line accounts for the noise introduced by the error term and reflects sample-specific estimates.\n",
    "\n",
    "# Null hypothesis for Simple Linear Regression:\n",
    "# H0: beta_1 = 0 (There is no linear association between X and Y)\n",
    "# This hypothesis states that the slope of the regression line is zero, implying that changes in X do not predict changes in Y.\n",
    "\n",
    "# Using the fitted model to assess the evidence against H0:\n",
    "# The p-value associated with the slope coefficient (beta_1) from model.summary().tables[1] indicates the strength of the evidence.\n",
    "# If the p-value is below a certain threshold (e.g., 0.05), we reject H0, suggesting there is a statistically significant linear relationship.\n",
    "\n",
    "# Interpretation for the Old Faithful Geyser dataset:\n",
    "# The analysis involves fitting a Simple Linear Regression model to the geyser data and checking if the p-value for beta_1 is low enough to reject the null hypothesis.\n",
    "# A low p-value suggests that the duration of eruptions (predictor) and the waiting time (outcome) have a significant linear relationship.\n",
    "\n",
    "# Restricting the dataset to short wait times and evaluating evidence against the null hypothesis:\n",
    "short_wait_limits = [62, 64, 66]\n",
    "for limit in short_wait_limits:\n",
    "    short_data = data[data[\"X\"] < limit]\n",
    "    short_model = smf.ols(formula='Y ~ X', data=short_data).fit()\n",
    "    print(f\"\\nSummary for wait times less than {limit} minutes:\")\n",
    "    print(short_model.summary())\n",
    "\n",
    "    # Plot the restricted data and fitted regression line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(short_data[\"X\"], short_data[\"Y\"], color='blue', label=f'Data for wait < {limit} min')\n",
    "    plt.plot(short_data[\"X\"], short_model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "    plt.title(f'Fitted Simple Linear Regression Model for Wait < {limit} min')\n",
    "    plt.xlabel('Predictor Variable (X)')\n",
    "    plt.ylabel('Outcome Variable (Y)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check the p-value for beta_1 to assess evidence against H0\n",
    "    p_value = short_model.pvalues['X']\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Evidence suggests a significant linear relationship for wait < {limit} min (p-value: {p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"No significant linear relationship detected for wait < {limit} min (p-value: {p_value:.4f})\")\n",
    "\n",
    "# Considering only long wait times (n=160) and analyzing the relationship\n",
    "long_wait_limit = 160\n",
    "long_data = data[data[\"X\"] >= long_wait_limit]\n",
    "long_model = smf.ols(formula='Y ~ X', data=long_data).fit()\n",
    "print(f\"\\nSummary for wait times greater than or equal to {long_wait_limit} minutes:\")\n",
    "print(long_model.summary())\n",
    "\n",
    "# Plot the long wait times data and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(long_data[\"X\"], long_data[\"Y\"], color='blue', label=f'Data for wait >= {long_wait_limit} min')\n",
    "plt.plot(long_data[\"X\"], long_model.fittedvalues, color='green', label='Fitted Regression Line')\n",
    "plt.title(f'Fitted Simple Linear Regression Model for Wait >= {long_wait_limit} min')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Check the p-value for beta_1 to assess evidence against H0 for long wait times\n",
    "p_value_long = long_model.pvalues['X']\n",
    "if p_value_long < 0.05:\n",
    "    print(f\"Evidence suggests a significant linear relationship for wait >= {long_wait_limit} min (p-value: {p_value_long:.4f})\")\n",
    "else:\n",
    "    print(f\"No significant linear relationship detected for wait >= {long_wait_limit} min (p-value: {p_value_long:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23155da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Add an indicator variable for short (< 68) and long (>= 68) wait times\n",
    "data['wait_category'] = np.where(data['X'] < 6.8, 'short', 'long')\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels with the indicator variable\n",
    "indicator_model = smf.ols(formula='Y ~ C(wait_category)', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model with the indicator variable\n",
    "print(indicator_model.summary())\n",
    "\n",
    "# Plot the data with different categories and fitted lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[data['wait_category'] == 'short'][\"X\"], data[data['wait_category'] == 'short'][\"Y\"], color='blue', label='Short Wait Times')\n",
    "plt.scatter(data[data['wait_category'] == 'long'][\"X\"], data[data['wait_category'] == 'long'][\"Y\"], color='orange', label='Long Wait Times')\n",
    "plt.axhline(y=indicator_model.params['Intercept'], color='green', linestyle='--', label='Fitted Line (Short Wait)')\n",
    "plt.axhline(y=indicator_model.params['Intercept'] + indicator_model.params['C(wait_category)[T.long]'], color='red', linestyle='-', label='Fitted Line (Long Wait)')\n",
    "plt.title('Fitted Simple Linear Regression Model with Indicator Variable for Wait Times')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Explanation:\n",
    "# The model includes an indicator variable (wait_category) that differentiates between short and long wait times.\n",
    "# The parameter `C(wait_category)[T.long]` represents the difference in the mean outcome (Y) for long wait times compared to short wait times.\n",
    "# The green dashed line represents the fitted value for short wait times, while the red solid line represents the fitted value for long wait times.\n",
    "# This helps us understand if there is a significant difference in the mean outcome based on the length of the wait time.\n",
    "\n",
    "# Interpretation of the p-value for `C(wait_category)[T.long]`:\n",
    "# - If the p-value is less than a certain threshold (e.g., 0.05), we conclude that there is a significant difference in the outcome (Y) between short and long wait times.\n",
    "# - Otherwise, we conclude that there is no significant difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Add an indicator variable for short (< 68) and long (>= 68) wait times\n",
    "data['wait_category'] = np.where(data['X'] < 6.8, 'short', 'long')\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels with the indicator variable\n",
    "indicator_model = smf.ols(formula='Y ~ C(wait_category)', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model with the indicator variable\n",
    "print(indicator_model.summary())\n",
    "\n",
    "# Plot the data with different categories and fitted lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[data['wait_category'] == 'short']['X'], data[data['wait_category'] == 'short']['Y'], color='blue', label='Short Wait Times')\n",
    "plt.scatter(data[data['wait_category'] == 'long']['X'], data[data['wait_category'] == 'long']['Y'], color='orange', label='Long Wait Times')\n",
    "plt.axhline(y=indicator_model.params['Intercept'], color='green', linestyle='--', label='Fitted Line (Short Wait)')\n",
    "plt.axhline(y=indicator_model.params['Intercept'] + indicator_model.params['C(wait_category)[T.long]'], color='red', linestyle='-', label='Fitted Line (Long Wait)')\n",
    "plt.title('Fitted Simple Linear Regression Model with Indicator Variable for Wait Times')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Explanation:\n",
    "# The model includes an indicator variable (wait_category) that differentiates between short and long wait times.\n",
    "# The parameter `C(wait_category)[T.long]` represents the difference in the mean outcome (Y) for long wait times compared to short wait times.\n",
    "# The green dashed line represents the fitted value for short wait times, while the red solid line represents the fitted value for long wait times.\n",
    "# This helps us understand if there is a significant difference in the mean outcome based on the length of the wait time.\n",
    "\n",
    "# Interpretation of the p-value for `C(wait_category)[T.long]`:\n",
    "# - If the p-value is less than a certain threshold (e.g., 0.05), we conclude that there is a significant difference in the outcome (Y) between short and long wait times.\n",
    "# - Otherwise, we conclude that there is no significant difference.\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = indicator_model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation of histograms and normality assumption:\n",
    "# - The histogram of residuals for the indicator model helps determine if the error terms are approximately normally distributed.\n",
    "# - If the histogram closely matches the overlaid normal distribution curve (red line), this suggests the plausibility of the normality assumption.\n",
    "# - If the histogram significantly deviates from the normal curve (e.g., skewed distribution, presence of outliers, or multimodal patterns), this indicates that the normality assumption may not hold.\n",
    "\n",
    "# Among the different models and histograms:\n",
    "# - The histogram that closely aligns with the normal distribution curve suggests the plausibility of the normality assumption.\n",
    "# - In the current histogram, the residuals appear to roughly match the normal curve, suggesting that the normality assumption may be reasonable for this model.\n",
    "# - However, if there are other histograms that show deviation from the normal curve, such as being asymmetrical, having heavy tails, or showing multiple peaks, these would indicate that the normality assumption is not supported for those models.\n",
    "# - Specifically, histograms that are skewed (either left or right), have obvious outliers, or have multiple modes do not support the assumption of normally distributed error terms, indicating a potential issue with the model fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation:\n",
    "# Simple Linear Regression (SLR) is a statistical method that models the relationship between a dependent variable (outcome) and an independent variable (predictor). \n",
    "# The equation for SLR is:\n",
    "#\n",
    "#     Y = beta_0 + beta_1 * X + epsilon\n",
    "#\n",
    "# Where:\n",
    "# - `Y` is the dependent variable (outcome).\n",
    "# - `X` is the independent variable (predictor).\n",
    "# - `beta_0` (intercept) is the value of `Y` when `X` is zero.\n",
    "# - `beta_1` (slope) indicates the change in `Y` for a one-unit change in `X`.\n",
    "# - `epsilon` is the error term that represents the difference between the actual and predicted values of `Y` and is assumed to be normally distributed.\n",
    "\n",
    "# In SLR, we assume that for given values of `X`, the `Y` values are drawn from a normal distribution centered around `beta_0 + beta_1 * X` with some variance.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters for the regression model\n",
    "beta_0 = 5  # Intercept\n",
    "beta_1 = 2  # Slope\n",
    "sigma = 1.5  # Standard deviation of the error term\n",
    "\n",
    "# Generate sample data\n",
    "X = np.linspace(0, 10, 100)  # Predictor variable\n",
    "epsilon = np.random.normal(0, sigma, len(X))  # Random error term\n",
    "Y = beta_0 + beta_1 * X + epsilon  # Outcome variable\n",
    "\n",
    "# Create a DataFrame for use with statsmodels\n",
    "data = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "\n",
    "# Add an indicator variable for short (< 68) and long (>= 68) wait times\n",
    "data['wait_category'] = np.where(data['X'] < 6.8, 'short', 'long')\n",
    "\n",
    "# Fit the Simple Linear Regression model using statsmodels with the indicator variable\n",
    "indicator_model = smf.ols(formula='Y ~ C(wait_category)', data=data).fit()\n",
    "\n",
    "# Print the summary of the fitted model with the indicator variable\n",
    "print(indicator_model.summary())\n",
    "\n",
    "# Plot the data with different categories and fitted lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data[data['wait_category'] == 'short']['X'], data[data['wait_category'] == 'short']['Y'], color='blue', label='Short Wait Times')\n",
    "plt.scatter(data[data['wait_category'] == 'long']['X'], data[data['wait_category'] == 'long']['Y'], color='orange', label='Long Wait Times')\n",
    "plt.axhline(y=indicator_model.params['Intercept'], color='green', linestyle='--', label='Fitted Line (Short Wait)')\n",
    "plt.axhline(y=indicator_model.params['Intercept'] + indicator_model.params['C(wait_category)[T.long]'], color='red', linestyle='-', label='Fitted Line (Long Wait)')\n",
    "plt.title('Fitted Simple Linear Regression Model with Indicator Variable for Wait Times')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Explanation:\n",
    "# The model includes an indicator variable (wait_category) that differentiates between short and long wait times.\n",
    "# The parameter `C(wait_category)[T.long]` represents the difference in the mean outcome (Y) for long wait times compared to short wait times.\n",
    "# The green dashed line represents the fitted value for short wait times, while the red solid line represents the fitted value for long wait times.\n",
    "# This helps us understand if there is a significant difference in the mean outcome based on the length of the wait time.\n",
    "\n",
    "# Interpretation of the p-value for `C(wait_category)[T.long]`:\n",
    "# - If the p-value is less than a certain threshold (e.g., 0.05), we conclude that there is a significant difference in the outcome (Y) between short and long wait times.\n",
    "# - Otherwise, we conclude that there is no significant difference.\n",
    "\n",
    "# Display a histogram of the error terms to show their normal distribution\n",
    "residuals = indicator_model.resid\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(residuals, bins=20, density=True, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot the normal distribution curve for comparison\n",
    "x_axis = np.linspace(min(residuals), max(residuals), 100)\n",
    "plt.plot(x_axis, norm.pdf(x_axis, 0, np.std(residuals)), color='red')\n",
    "plt.title('Histogram of Residuals with Normal Distribution Curve')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation of histograms and normality assumption:\n",
    "# - The histogram of residuals for the indicator model helps determine if the error terms are approximately normally distributed.\n",
    "# - If the histogram closely matches the overlaid normal distribution curve (red line), this suggests the plausibility of the normality assumption.\n",
    "# - If the histogram significantly deviates from the normal curve (e.g., skewed distribution, presence of outliers, or multimodal patterns), this indicates that the normality assumption may not hold.\n",
    "\n",
    "# Among the different models and histograms:\n",
    "# - The histogram that closely aligns with the normal distribution curve suggests the plausibility of the normality assumption.\n",
    "# - In the current histogram, the residuals appear to roughly match the normal curve, suggesting that the normality assumption may be reasonable for this model.\n",
    "# - However, if there are other histograms that show deviation from the normal curve, such as being asymmetrical, having heavy tails, or showing multiple peaks, these would indicate that the normality assumption is not supported for those models.\n",
    "# - Specifically, histograms that are skewed (either left or right), have obvious outliers, or have multiple modes do not support the assumption of normally distributed error terms, indicating a potential issue with the model fit.\n",
    "\n",
    "# Two-sample hypothesis testing using permutation test and bootstrap confidence interval\n",
    "# Separate the data into two groups: short and long wait times\n",
    "short_wait_data = data[data['wait_category'] == 'short']['Y']\n",
    "long_wait_data = data[data['wait_category'] == 'long']['Y']\n",
    "\n",
    "# Permutation test to assess the difference in means\n",
    "observed_diff = long_wait_data.mean() - short_wait_data.mean()\n",
    "\n",
    "# Permutation test function\n",
    "def permutation_test(data1, data2, num_permutations=10000):\n",
    "    combined = np.concatenate([data1, data2])\n",
    "    count = 0\n",
    "    for _ in range(num_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        new_data1 = combined[:len(data1)]\n",
    "        new_data2 = combined[len(data1):]\n",
    "        new_diff = new_data2.mean() - new_data1.mean()\n",
    "        if abs(new_diff) >= abs(observed_diff):\n",
    "            count += 1\n",
    "    return count / num_permutations\n",
    "\n",
    "p_value_permutation = permutation_test(short_wait_data.values, long_wait_data.values)\n",
    "print(f\"Permutation test p-value: {p_value_permutation:.4f}\")\n",
    "\n",
    "# Bootstrap confidence interval for the difference in means\n",
    "num_bootstrap_samples = 10000\n",
    "diff_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    short_sample = np.random.choice(short_wait_data, size=len(short_wait_data), replace=True)\n",
    "    long_sample = np.random.choice(long_wait_data, size=len(long_wait_data), replace=True)\n",
    "    diff_means.append(long_sample.mean() - short_sample.mean())\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "lower_bound = np.percentile(diff_means, 2.5)\n",
    "upper_bound = np.percentile(diff_means, 97.5)\n",
    "print(f\"95% Bootstrap Confidence Interval for Difference in Means: ({lower_bound:.4f}, {upper_bound:.4f})\")\n",
    "\n",
    "# Interpretation:\n",
    "# - The permutation test p-value helps determine if there is a significant difference between the means of the two groups.\n",
    "# - A p-value less than 0.05 suggests that the difference in means is statistically significant.\n",
    "# - The bootstrap confidence interval provides an estimate of the range within which the true difference in means lies with 95% confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
