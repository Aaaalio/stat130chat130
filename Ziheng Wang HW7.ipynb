{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91749f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Difference Between Simple Linear Regression and Multiple Linear Regression\n",
    "Simple Linear Regression models the relationship between a dependent variable and a single independent variable, creating a straight-line fit. It predicts outcomes based on one predictor.\n",
    "Multiple Linear Regression extends this to include two or more independent variables, allowing the model to capture more complex relationships by considering additional factors.\n",
    "Benefit: Multiple Linear Regression can provide more accurate predictions because it captures the influence of multiple variables rather than just one.\n",
    "\n",
    "2. Difference Between Continuous and Indicator Variables in Simple Linear Regression\n",
    "Continuous Variable: A variable that can take any value within a range, like temperature or age. In Simple Linear Regression, it predicts the dependent variable as a linear function of this continuous predictor.\n",
    "Indicator Variable: A binary variable, often representing categories (e.g., 0 or 1 for “no” or “yes”), used to model categorical distinctions in Simple Linear Regression. Here, it predicts the outcome based on the presence or absence of a condition.\n",
    "Linear Forms:\n",
    "\n",
    "With a continuous variable, the model produces a line with a specific slope, estimating a continuous relationship.\n",
    "With an indicator variable, the model produces two flat sections (like an intercept shift) without any slope, distinguishing between the two indicator levels.\n",
    "3. Behavior Change When Introducing a Single Indicator Variable in Multiple Linear Regression\n",
    "Adding an indicator variable alongside a continuous variable in Multiple Linear Regression results in a model that fits two parallel lines. The continuous predictor still defines the slope, while the indicator variable shifts the intercept up or down depending on its level.\n",
    "Linear Forms:\n",
    "\n",
    "Simple Linear Regression with one continuous variable produces a single line.\n",
    "Multiple Linear Regression with both a continuous and an indicator variable yields two parallel lines, each at different intercepts based on the indicator variable level.\n",
    "4. Effect of Adding an Interaction Between a Continuous and Indicator Variable in Multiple Linear Regression\n",
    "Adding an interaction term between a continuous and an indicator variable allows the slope of the continuous variable to vary between the levels of the indicator variable. This means that each level of the indicator now has a unique line with its own slope.\n",
    "Linear Form:\n",
    "\n",
    "The model captures different relationships (slopes) for each level of the indicator variable, allowing for more nuanced interpretations of how the continuous predictor impacts the outcome depending on the indicator's level.\n",
    "5. Behavior of Multiple Linear Regression Model Based Only on Indicator Variables from a Non-Binary Categorical Variable\n",
    "When a categorical variable with more than two levels is used in Multiple Linear Regression, it is encoded into binary (dummy) variables. Each dummy variable represents one level of the categorical variable (except for one level as the reference).\n",
    "Linear Form:\n",
    "\n",
    "The model will produce parallel flat sections corresponding to each category level. The absence of a continuous variable means there is no slope; the model only adjusts intercepts based on category membership.\n",
    "Binary Variable Encodings:\n",
    "\n",
    "These are the dummy variables (e.g., for a three-level categorical variable, two binary variables are created, with the third level as a reference). Each encoding allows the model to adjust the intercept to account for each categorical level's effect independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Identifying the Outcome and Predictor Variables\n",
    "Outcome Variable: This is the main variable we aim to predict, often reflecting what we're most interested in understanding or forecasting (like revenue, test scores, or temperature).\n",
    "Predictor Variables: These are the factors we believe might influence the outcome. They can be continuous (like age or income) or categorical (like gender or region).\n",
    "Given your scenario, we would identify which variables most likely serve as predictors and which one acts as the outcome. If you provide specific details, I can help clarify which variables fall into each category.\n",
    "\n",
    "2. Considering Potential Interactions\n",
    "Interactions: Sometimes, the effect of one predictor on the outcome may change depending on the level of another predictor. For example, in a sales model, the effect of marketing spend might differ depending on the season. This is where interactions come in—they allow us to model the combined effect of predictors.\n",
    "When assessing interactions, we consider whether it’s realistic for predictors to work together in influencing the outcome rather than independently.\n",
    "\n",
    "3. Linear Forms Without and With Interactions\n",
    "Without Interaction:\n",
    "\n",
    "The basic model form here is:\n",
    "Outcome = Constant + (Effect of Predictor 1) + (Effect of Predictor 2) + ... + Error\n",
    "This form assumes each predictor affects the outcome independently, without any combined effects.\n",
    "With Interaction:\n",
    "\n",
    "If there's a meaningful interaction between two predictors, we add an interaction term:\n",
    "Outcome = Constant + (Effect of Predictor 1) + (Effect of Predictor 2) + (Effect of Predictor 1 and Predictor 2 Together) + Error\n",
    "In this form, the interaction term allows the influence of one predictor on the outcome to depend on the level of the other predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a96a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "To fit multiple linear regression models to your dataset using statsmodels.formula.api (imported as smf), we’ll need to follow these steps:\n",
    "\n",
    "Load the dataset: Ensure you have the data loaded into a pandas DataFrame.\n",
    "Define the model: Specify the model formula, selecting the outcome variable and the predictor variables you want to include. If there are any interactions you want to consider, they can be specified in the formula.\n",
    "Fit the model: Use smf.ols() to create the model and .fit() to estimate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "The apparent contradiction here arises from a common nuance in interpreting multiple linear regression results: low overall model fit (R-squared) versus strong evidence of individual predictor effects (significant coefficients). Let’s unpack why both statements can coexist and make sense in context:\n",
    "\n",
    "Model Fit (R-squared): When you say \"the model only explains 17.6% of the variability in the data,\" this is referring to the R-squared value, which indicates the proportion of variance in the outcome that the model’s predictors collectively explain. An R-squared of 17.6% suggests that most of the variability in the outcome is due to factors not captured by this model. It doesn’t mean the model is useless; rather, it means there’s likely a complex underlying relationship in the data or missing variables that could better explain the outcome.\n",
    "\n",
    "Significant Coefficients: The statement that \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\" suggests that individual predictors in the model are statistically significant—meaning their estimated effects are unlikely to be due to chance. Large coefficient values and small p-values indicate that these predictors have a substantial and statistically significant relationship with the outcome, even if this does not translate into a high R-squared value.\n",
    "\n",
    "Reconciling the Contradiction\n",
    "The key here is that significant individual predictors don’t always translate into a high R-squared. Some possible reasons include:\n",
    "\n",
    "Other Influential Factors Are Missing: The model may lack other important variables, leaving much of the outcome unexplained by the predictors in the model.\n",
    "High Variability in the Outcome: If the outcome is inherently variable or influenced by many subtle factors, even strong predictors might not capture most of the variance.\n",
    "Specific Effects vs. Overall Fit: Statistically significant coefficients indicate that individual predictors have a real effect on the outcome, but these effects might be smaller relative to the overall variance, leading to a modest R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9590df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell 1: Importing Libraries and Loading Data\n",
    "python\n",
    "复制代码\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Loading the dataset (assuming a CSV file format)\n",
    "data = pd.read_csv('canadian_social_connection_survey.csv')\n",
    "data.head()\n",
    "Discussion\n",
    "This cell imports the necessary libraries and loads the dataset into a pandas DataFrame. We would check the first few rows of the data to understand the variables available and their types (e.g., continuous, categorical). Seeing the data can help identify potential outcome and predictor variables.\n",
    "\n",
    "Cell 2: Defining the Multiple Linear Regression Model (without Interactions)\n",
    "python\n",
    "复制代码\n",
    "# Fitting a model without interaction terms\n",
    "model_no_interaction = smf.ols('Outcome ~ Predictor1 + Predictor2 + Predictor3', data=data).fit()\n",
    "print(model_no_interaction.summary())\n",
    "Discussion\n",
    "This cell defines a basic multiple linear regression model with Outcome as the dependent variable and Predictor1, Predictor2, and Predictor3 as predictors. By omitting interaction terms, we assume each predictor’s effect on the outcome is independent of the others. The summary output will provide insights into:\n",
    "\n",
    "R-squared (model fit),\n",
    "Coefficient estimates (the effect size of each predictor),\n",
    "P-values (significance of predictors).\n",
    "If, for example, we observe a low R-squared with significant coefficients (aligning with our previous discussion), it indicates that the model captures some individual relationships but doesn’t explain much of the overall variability.\n",
    "\n",
    "Cell 3: Adding Interaction Terms to the Model\n",
    "python\n",
    "复制代码\n",
    "# Fitting a model with interaction terms between Predictor1 and Predictor2\n",
    "model_with_interaction = smf.ols('Outcome ~ Predictor1 * Predictor2 + Predictor3', data=data).fit()\n",
    "print(model_with_interaction.summary())\n",
    "Discussion\n",
    "Here, we add an interaction term between Predictor1 and Predictor2 by using the * operator, which includes both the main effects of these predictors and their interaction. This allows us to examine if the effect of Predictor1 on the outcome changes based on the level of Predictor2.\n",
    "\n",
    "Interaction Effect: If the interaction term is significant, it means the relationship between Predictor1 and the outcome depends on the level of Predictor2.\n",
    "Model Fit Comparison: By comparing the R-squared from this model to the previous one, we can determine if including interactions improves the model's ability to explain variability.\n",
    "Cell 4: Visualizing Model Residuals\n",
    "python\n",
    "复制代码\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting residuals to check model fit\n",
    "plt.scatter(model_with_interaction.fittedvalues, model_with_interaction.resid)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.show()\n",
    "Discussion\n",
    "In this cell, we generate a residual plot to examine the fit quality. The residual plot helps diagnose whether the model's assumptions are valid (e.g., constant variance of residuals, linear relationship). Ideally, residuals should be randomly scattered around zero. Patterns in the residuals (like a funnel shape) might suggest model misfit or the need for additional transformations.\n",
    "\n",
    "Cell 5: Comparing Model Performance Metrics\n",
    "python\n",
    "复制代码\n",
    "# Printing out R-squared and adjusted R-squared for comparison\n",
    "print(f'R-squared without interaction: {model_no_interaction.rsquared:.3f}')\n",
    "print(f'R-squared with interaction: {model_with_interaction.rsquared:.3f}')\n",
    "print(f'Adjusted R-squared without interaction: {model_no_interaction.rsquared_adj:.3f}')\n",
    "print(f'Adjusted R-squared with interaction: {model_with_interaction.rsquared_adj:.3f}')\n",
    "Discussion\n",
    "In this final cell, we compare R-squared and adjusted R-squared values between the models with and without interactions. Adjusted R-squared is particularly helpful when comparing models, as it adjusts for the number of predictors, offering a more accurate reflection of the model's explanatory power.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "If the R-squared increases significantly with the interaction model, it suggests that adding interactions better explains the outcome's variability.\n",
    "If R-squared remains low even with interactions, it may indicate that additional, relevant predictors are missing, or the model structure is too simplistic for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. The Design Matrix and Model Specification\n",
    "Design Matrix (model4_spec.exog): In linear regression, the design matrix is a structured table of all predictor variables (columns) and observations (rows) used to predict the outcome variable. This matrix includes all terms in the model, including main effects (individual predictors) and any interaction terms.\n",
    "Linear Form of model4: When we specify a model with interactions or polynomial terms, the software automatically creates new columns in the design matrix to represent these terms. For example, if model4 includes predictors X1, X2, and an interaction term X1*X2, the design matrix will include three columns: X1, X2, and X1*X2.\n",
    "2. Multicollinearity in the Design Matrix\n",
    "What is Multicollinearity? Multicollinearity occurs when predictor variables in the design matrix are highly correlated with each other. This is often seen with interaction terms or polynomial terms because they are derived from existing predictors and are therefore not fully independent.\n",
    "Checking for Multicollinearity: Using a function like np.corrcoef(model4_spec.exog) allows us to examine correlations between predictors in the design matrix. High correlations indicate multicollinearity, which means that some predictors don’t add independent information.\n",
    "3. Effects of Multicollinearity on Model Performance\n",
    "Parameter Instability: Multicollinearity makes it difficult to estimate unique effects for each predictor since they contain overlapping information. This can lead to inflated coefficient estimates and unstable parameters.\n",
    "Reduced Generalization: Because of this instability, models with high multicollinearity often perform poorly on new, unseen data (lack of \"out-of-sample\" generalization). Essentially, the model may appear to fit the training data well but fail to predict accurately for other datasets, as it has \"overfit\" on redundant patterns specific to the training data.\n",
    "Summary Explanation in My Own Words\n",
    "The design matrix in model4 contains all predictors and interaction terms as columns, creating potential multicollinearity if predictors are highly correlated. This multicollinearity means that the model's coefficients are unstable and sensitive to small changes in the data, resulting in poor generalization. In other words, while model4 might fit the data it was trained on, it’s likely to make unreliable predictions on new data due to the overlapping and redundant information among predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf744d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "From model3_fit to model4_fit\n",
    "Principle of Adding Complexity: Moving from model3_fit to model4_fit typically involves adding new terms, such as interaction terms or polynomial terms, to capture more complex relationships in the data. model3_fit might include only main effects, while model4_fit adds interactions or nonlinear terms to address relationships that model3 couldn't fully capture.\n",
    "Rationale: The goal is to improve the model’s ability to explain the outcome by incorporating predictors that interact or have nonlinear relationships, potentially increasing the model’s fit to the data.\n",
    "2. From model4_fit to model5_linear_form\n",
    "Principle of Refinement: As the model progresses to model5, the approach may focus on refining the fit by selectively including meaningful interactions or transformations that improve the model’s explanatory power without over-complicating it. This might involve adjusting which terms to keep, removing weak predictors, or adding polynomial terms with caution.\n",
    "Rationale: The aim here is to balance complexity with stability, refining interactions or nonlinear terms that proved helpful in model4_fit to improve both in-sample fit and potential generalizability.\n",
    "3. From model5_linear_form to model6_linear_form\n",
    "Principle of Systematic Expansion: Moving from model5 to model6 may involve systematically expanding the model by adding additional predictors or interaction terms informed by domain knowledge. By this stage, any new terms added are carefully chosen to capture remaining variance in the outcome.\n",
    "Rationale: The purpose is to maximize the model’s fit with new, thoughtfully selected predictors, ideally minimizing any unnecessary complexity while addressing gaps or underexplored aspects identified in model5.\n",
    "4. From model6_linear_form to model7_linear_form\n",
    "Principle of Final Optimization: model7 is likely a final, optimized version of the model, building on model6 by only adding terms or making modifications that offer a clear, additional predictive benefit. This step may involve fine-tuning based on statistical criteria, removing collinear terms, or using regularization if needed.\n",
    "Rationale: This last step solidifies the model structure, keeping only the most valuable predictors to ensure stability, interpretability, and generalizability for predictions on new data.\n",
    "Summary Explanation in My Own Words\n",
    "Each model builds progressively on the last by adding complexity in a structured way—first through interactions and nonlinear terms to capture more complex relationships, then by refining and expanding the predictors to improve the model's fit, and finally by optimizing for a stable and generalizable structure. The extensions are guided by the principle of balancing complexity with interpretability and predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "To create and visualize multiple paired “in-sample” and “out-of-sample” model performance metrics, we’ll write a for loop that:\n",
    "\n",
    "Splits the Data Randomly: Each iteration will create a new random split of the data into training (in-sample) and test (out-of-sample) sets.\n",
    "Fits the Model on the Training Set: We’ll fit a specified model on the training set.\n",
    "Evaluates Performance: For each split, we’ll compute performance metrics for both the in-sample (training) and out-of-sample (test) sets.\n",
    "Visualizes Performance Metrics: We’ll plot the results to observe how in-sample and out-of-sample performance varies across splits, highlighting the potential for overfitting or instability in the model.\n",
    "Here’s how the code might look:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data setup (assuming df is your DataFrame with predictors X and outcome y)\n",
    "X = df[['Predictor1', 'Predictor2', 'Predictor3']]\n",
    "y = df['Outcome']\n",
    "\n",
    "# Storage for performance metrics\n",
    "in_sample_errors = []\n",
    "out_of_sample_errors = []\n",
    "\n",
    "# Run multiple iterations without setting a random seed\n",
    "for i in range(50):  # Adjust 50 to desired number of iterations\n",
    "    # Step 1: Randomly split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    # Step 2: Initialize and fit the model on the training set\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Calculate performance metrics\n",
    "    # In-sample error\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    in_sample_error = mean_squared_error(y_train, y_train_pred)\n",
    "    in_sample_errors.append(in_sample_error)\n",
    "    \n",
    "    # Out-of-sample error\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    out_of_sample_error = mean_squared_error(y_test, y_test_pred)\n",
    "    out_of_sample_errors.append(out_of_sample_error)\n",
    "\n",
    "# Step 4: Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(50), in_sample_errors, label=\"In-Sample Error\", marker='o')\n",
    "plt.plot(range(50), out_of_sample_errors, label=\"Out-of-Sample Error\", marker='x')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"In-Sample vs. Out-of-Sample Error Across Multiple Random Splits\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "Explanation of the Results and Purpose of the Demonstration\n",
    "Purpose: This demonstration reveals the variability in model performance due to randomness in data splitting, allowing us to observe trends in in-sample versus out-of-sample errors. By not setting a fixed random seed, we see genuine variation, which is key for understanding model stability and potential overfitting.\n",
    "\n",
    "Expected Results: Typically, in-sample errors (training errors) will be lower than out-of-sample errors (test errors) since the model is optimized to fit the training data. If the gap between in-sample and out-of-sample errors is consistently large, it may indicate that the model is overfitting—capturing noise in the training data rather than general patterns.\n",
    "\n",
    "Interpretation: If we see that out-of-sample errors vary widely across iterations, it suggests that the model’s predictive power on new data is unstable, perhaps due to high sensitivity to different training samples or multicollinearity in the design matrix. Consistently small differences between in-sample and out-of-sample errors suggest a more robust model with better generalization.\n",
    "\n",
    "This process helps clarify the importance of evaluating models on multiple random splits to ensure reliable performance, highlighting the potential pitfalls of overfitting and instability in real-world predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c127566",
   "metadata": {},
   "outputs": [],
   "source": [
    "This output illustrates an analysis where multiple models are fit to different subsets of a dataset (pokeaman) across different generations of Pokémon (referred to as Generation 1 to Generation 6). The goal here is to observe the in-sample and out-of-sample performance (R-squared values) of these models under various conditions. Let's go through what each step means and the insights we can gain.\n",
    "\n",
    "Step-by-Step Explanation of Results\n",
    "Original Model (model7_fit and model6_fit):\n",
    "\n",
    "In-sample R-squared: This value measures the proportion of variance in the outcome variable HP that each original model explains within the training set.\n",
    "Out-of-sample R-squared: This R-squared measures the model's performance on a held-out test set (pokeaman_test). Lower out-of-sample R-squared values compared to in-sample ones suggest some degree of overfitting.\n",
    "Model Trained on Generation 1 Only (model7_gen1_predict_future_fit and model6_gen1_predict_future_fit):\n",
    "\n",
    "In-sample R-squared (gen1_predict_future): By training the model solely on Generation 1 data, we observe how well the model fits this subset. The in-sample R-squared is relatively high (e.g., 0.572 for model7_gen1_predict_future_fit), suggesting a good fit within Generation 1.\n",
    "Out-of-sample R-squared (gen1_predict_future): When predicting on data from other generations (Generation 2 and onward), the out-of-sample R-squared drops significantly (e.g., to 0.111 for model7). This drop indicates that a model trained exclusively on Generation 1 does not generalize well to other generations, possibly due to differences between Generation 1 and the later data.\n",
    "Model Trained on Generations 1 to 5 Only (model7_gen1to5_predict_future_fit and model6_gen1to5_predict_future_fit):\n",
    "\n",
    "In-sample R-squared (gen1to5_predict_future): Here, training includes more data (all generations except Generation 6), resulting in a moderately high in-sample R-squared (e.g., 0.390 for model7), but slightly lower than when fitting solely to Generation 1. This may reflect an attempt to fit a more diverse dataset.\n",
    "Out-of-sample R-squared (gen1to5_predict_future): When tested on Generation 6 data, the out-of-sample R-squared is still lower than the in-sample value but not as low as when training only on Generation 1 (e.g., 0.234 for model7). This suggests that training on a broader set of generations improves generalization to unseen data (like Generation 6).\n",
    "Key Insights\n",
    "Model Generalization Across Generations:\n",
    "\n",
    "The models trained on broader datasets (generations 1 to 5) show better generalization to out-of-sample generations (like Generation 6) compared to those trained on a single generation (Generation 1). This suggests that including more representative data in training helps improve predictive accuracy on new, unseen groups.\n",
    "Overfitting and Dataset Diversity:\n",
    "\n",
    "High in-sample R-squared values with significantly lower out-of-sample R-squared values indicate overfitting, where the model is too closely fitted to specific characteristics of the training data and thus struggles to generalize. Training on a diverse dataset (generations 1 to 5) appears to reduce this effect, as seen in the smaller gap between in-sample and out-of-sample R-squared.\n",
    "Purpose of This Demonstration:\n",
    "\n",
    "This exercise demonstrates how model performance depends on the diversity and representativeness of the training data. Training on only one generation leads to high specificity but poor generalizability, whereas training on multiple generations achieves a more balanced fit. This is crucial for any predictive modeling task where the goal is to apply the model to new or broader populations, not just those seen in training.\n",
    "This illustration shows how different data choices for training affect model performance. When models are trained on just one generation, they fit that generation well but perform poorly on others. When trained on multiple generations, the model shows better balance, with less overfitting and better predictions on unseen data. This highlights the importance of using diverse, representative data in training for building models that generalize well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
